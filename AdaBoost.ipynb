{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sklearn.datasets\n",
    "from sklearn.svm import SVC\n",
    "import sys\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = sklearn.datasets.make_hastie_10_2()\n",
    "X_train = X[0:8000,:]\n",
    "y_train = y[0:8000]\n",
    "X_test = X[8000:,:]\n",
    "y_test = y[8000:]\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "1. Implement the AdaBoost ensemble algorithm by completing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self, weakModel, T):\n",
    "        \n",
    "        self.weakModel = weakModel\n",
    "        self.T = T\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y, K):\n",
    "        \n",
    "        sample_weight = np.ones(len(X)) / len(X)\n",
    "        \n",
    "        for t in range(0,self.T):\n",
    "            \n",
    "            #Deep copy del modello\n",
    "            current_model = copy.deepcopy(self.weakModel)\n",
    "            \n",
    "            #Train del modello con i pesi specifici\n",
    "            current_model.fit(X, y, sample_weight=sample_weight)\n",
    "            \n",
    "            #Predizione\n",
    "            y_pred = current_model.predict(X)\n",
    "\n",
    "            #Calcolo dell'errore\n",
    "            error = sum(sample_weight * [1 if yy_pred != y[idx] else 0 for idx, yy_pred in enumerate(y_pred)])\n",
    "            \n",
    "            #Affidabilità del modello\n",
    "            alfa = np.log((1 - error) / error) / 2\n",
    "            \n",
    "            #Aggiornamento dei pesi\n",
    "            for i in range(len(sample_weight)):\n",
    "                sample_weight[i] = sample_weight[i] * np.exp(-alfa * y_pred[i] * y[i])\n",
    "                \n",
    "            #Normalizzazione\n",
    "            sample_weight = sample_weight / sum(sample_weight)\n",
    "            \n",
    "            #Memorizzzione del modello con il suo alfa\n",
    "            self.models.append([alfa, current_model])\n",
    "            \n",
    "            #Stampa dello score del modello corrente\n",
    "            if t%K == 0:\n",
    "                print(\"Iterazione \", t)\n",
    "                print(\"Loss corrente: \" + str(error))\n",
    "                print(\"Alfa del modello corrente: \" + str(alfa))\n",
    "                \n",
    "                print(\"Score di Adaboost \")\n",
    "                y_pred = self.predict(X)\n",
    "                self.print_score(y, y_pred)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        #Vettore contenente la somma delle predizioni dei modelli\n",
    "        y_sum_of_prediction = np.zeros(len(X))\n",
    "        \n",
    "        for i in range(len(self.models)):\n",
    "            \n",
    "            current_model = self.models[i][1]\n",
    "            alfa = self.models[i][0]\n",
    "\n",
    "            # add to the final prediction, the weighted prediction of current model, weighted by its reliability alfa\n",
    "            y_sum_of_prediction = np.add(y_sum_of_prediction, current_model.predict(X) * alfa)\n",
    "        \n",
    "        #Funzione step per ritornare per ogni istanza 1 o -1    \n",
    "        return [1 if y >= 0 else -1 for y in y_sum_of_prediction]\n",
    "    \n",
    "    def print_score(self, y_actual, y_predicted):\n",
    "        \n",
    "        n_samples = len(y_predicted)\n",
    "        \n",
    "        errors = (1 - np.dot(y_actual, y_predicted) / n_samples) / 2\n",
    "\n",
    "        print(\"% di errori: \" + str(errors * 100) + \" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation you are free to assume:\n",
    "- that the problem is a binary classification problem with labels in $\\{-1, +1\\}$.\n",
    "- that the weakModel can fit a weighted sample set by means of the call `weakModel.fit(X,y,sample_weight=w)` where `w` is a vector of length $|y|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Test your implementation on the dataset loaded above and using an SVC with a polynomial kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. evaluate the AdaBoost performances as usual by calculating the classification error and compare it with the classification error of the weak model.\n",
    "\n",
    "**Note 1**:  \n",
    "since the labels are bound to be in ${+1, -1}$, the classification error (i.e., the number of incorrectly classified examples over the total number of examples) can be easily computed as:\n",
    "$$\n",
    "   error(y,y') = \\frac{N - y \\cdot y'}{2N} = \\frac{1}{2} - \\frac{y \\cdot y'}{2N},\n",
    "$$\n",
    "where $N$ is the total number of examples. The formula can be derived noticing that $y \\cdot y'$ calculates the number $N_c$ of examples correctly classified  minus the number $N_{\\bar c}$ of examples incorrectly classified. We have then $y \\cdot y' = N_c - N_{\\bar c}$ and by noticing that $N = N_c + N_{\\bar c}$:\n",
    "$$\n",
    "   N - y \\cdot y' = N_c + N_{\\bar c} - N_c + N_{\\bar c} = 2 N_{\\bar c} \\Rightarrow \\frac{N - y \\cdot y'}{2 N} = \\frac{N_{\\bar c}}{N}\n",
    "$$\n",
    "\n",
    "**Note 2**:\n",
    "do not forget to deepcopy your base model before fitting it to the new data\n",
    "\n",
    "**Note 3**:\n",
    "The SVC model allows specifying weights, but it *does not* work well when weights are normalized (it works well when the weights are larger). The following class takes normalized weights and denormalize them before passing them to the SVC classifier:\n",
    "\n",
    "```python\n",
    "    class SVC_:\n",
    "        def __init__(self, kernel=\"rbf\", degree=\"3\"):\n",
    "            self.svc = SVC(kernel=kernel, degree=degree)\n",
    "\n",
    "        def fit(self, X,y,sample_weight=None):\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = sample_weight * len(X)\n",
    "\n",
    "            self.svc.fit(X,y,sample_weight=sample_weight)\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.svc.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVC_:\n",
    "    def __init__(self, kernel=\"rbf\", degree=\"3\", coef0=0):\n",
    "        self.svc = SVC(kernel=kernel, degree=degree, coef0=coef0, gamma=\"scale\")\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = sample_weight * len(X)\n",
    "\n",
    "        self.svc.fit(X, y, sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.svc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterazione  0\n",
      "Loss corrente: 0.3509999999999752\n",
      "Alfa del modello corrente: 0.3073232466191665\n",
      "Score di Adaboost \n",
      "% di errori: 35.099999999999994 \n",
      "\n",
      "Iterazione  10\n",
      "Loss corrente: 0.38451750215342967\n",
      "Alfa del modello corrente: 0.23520860119407447\n",
      "Score di Adaboost \n",
      "% di errori: 20.85 \n",
      "\n",
      "Iterazione  20\n",
      "Loss corrente: 0.3938554075842484\n",
      "Alfa del modello corrente: 0.215567351419645\n",
      "Score di Adaboost \n",
      "% di errori: 16.4125 \n",
      "\n",
      "Iterazione  30\n",
      "Loss corrente: 0.4101405652505821\n",
      "Alfa del modello corrente: 0.18169215965558866\n",
      "Score di Adaboost \n",
      "% di errori: 14.687499999999998 \n",
      "\n",
      "Iterazione  40\n",
      "Loss corrente: 0.44025400809773974\n",
      "Alfa del modello corrente: 0.1200656217320331\n",
      "Score di Adaboost \n",
      "% di errori: 14.075 \n",
      "\n",
      "Iterazione  50\n",
      "Loss corrente: 0.45221816543799837\n",
      "Alfa del modello corrente: 0.09585618264147742\n",
      "Score di Adaboost \n",
      "% di errori: 13.424999999999997 \n",
      "\n",
      "Iterazione  60\n",
      "Loss corrente: 0.4465898878164908\n",
      "Alfa del modello corrente: 0.1072293217306829\n",
      "Score di Adaboost \n",
      "% di errori: 12.8 \n",
      "\n",
      "Iterazione  70\n",
      "Loss corrente: 0.4664788863510456\n",
      "Alfa del modello corrente: 0.06714294305879458\n",
      "Score di Adaboost \n",
      "% di errori: 12.75 \n",
      "\n",
      "Iterazione  80\n",
      "Loss corrente: 0.45410998182437406\n",
      "Alfa del modello corrente: 0.09203905206206871\n",
      "Score di Adaboost \n",
      "% di errori: 12.25 \n",
      "\n",
      "Iterazione  90\n",
      "Loss corrente: 0.4528194783824132\n",
      "Alfa del modello corrente: 0.09464261281146863\n",
      "Score di Adaboost \n",
      "% di errori: 12.362499999999999 \n",
      "\n",
      "Score sul training set\n",
      "% di errori: 11.887500000000001 \n",
      "\n",
      "Score sul test set\n",
      "% di errori: 15.000000000000002 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "weakModel= SVC_(kernel=\"poly\", degree=3, coef0=0)\n",
    "adaboost = AdaBoost(weakModel, 100).fit(X_train,y_train,10)\n",
    "y_train_predicted = adaboost.predict(X_train)\n",
    "y_test_predicted  = adaboost.predict(X_test)\n",
    "\n",
    "print(\"Score sul training set\")\n",
    "\n",
    "adaboost.print_score(y_train, y_train_predicted)\n",
    "\n",
    "print(\"Score sul test set\")\n",
    "adaboost.print_score(y_test, y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a weak learner to be used with the AdaBoost algorithm you just wrote. The weak learner that you will implement is the most inaccurate weak learner possible: it basically works by extracting a linear model at random and trying to use that model to classify the examples. Being extracted at random the models it generates do not guarantee that the weighted error $\\epsilon_t$ is smaller than $0.5$. The algorithm solves this problem by flipping the decisions whenever it finds out that $\\epsilon_t > 0.5$ (i.e., if the weighted error is larger than $0.5$ it reverses the sign of all the weights so that the decision surface stays the same, but the regions where it predicts $+1$ and $-1$ are reversed).\n",
    "\n",
    "    It shall work as follows:\n",
    "\n",
    "    - it creates a random linear model by generating the needed weight vector $\\mathbf{w}$ at random (**note**: these are the weights of the linear model, they are *NOT* related in any way to the weights of the examples); each weight shall be sampled from U(-1,1);\n",
    "    - it evaluates the weighted loss $\\epsilon_t$ on the given dataset and flip the linear model if $\\epsilon_t > 0.5$;\n",
    "    - at prediction time it predicts +1 if $\\mathbf{x} \\cdot \\mathbf{w} > 0$; it predicts -1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLinearModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w = []\n",
    "        self.t = 0\n",
    "    \n",
    "    def loss(self, y, y_predicted, example_weights):\n",
    "        \n",
    "        return sum(example_weights * [1 if y_predicted != y[idx] else 0 for idx, y_predicted in enumerate(y_predicted)])\n",
    "        \n",
    "    def fit(self,X,y, sample_weight=None):\n",
    "        \n",
    "        numero_di_features = np.shape(X)[1]\n",
    "        \n",
    "        #Generazione valori random per w e t\n",
    "        for i in range(numero_di_features):\n",
    "            self.w.append(random.uniform(-1, 1))\n",
    "\n",
    "        self.w = np.array(self.w)\n",
    "        self.t = random.uniform(-1, 1)\n",
    "\n",
    "        y_predicted = self.predict(X)\n",
    "\n",
    "        #Flip dei parametri del modello se la loss è abbastanza grande\n",
    "        if self.loss(y, y_predicted, sample_weight) > 0.5:\n",
    "            self.w = self.w * -1\n",
    "            self.t = self.t * -1\n",
    "\n",
    "        return self       \n",
    "        \n",
    "    def predict(self,X):\n",
    "        #Ritorna -1 o 1 in base al segno della classificazione\n",
    "        return np.array([-1 if y < 0 else 1 for y in (np.dot(X, self.w) + self.t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Learn an AdaBoost model using the RandomLinearModel weak learner printing every $K$ iterations the weighted error and the current error of the ensemble (you are free to choose $K$ so to make your output just frequent enough to let you know what is happening but without flooding the console with messages). Evaluate the training and test error of the final ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterazione  0\n",
      "Loss corrente: 0.4976249999999591\n",
      "Alfa del modello corrente: 0.004750035724523781\n",
      "Score di Adaboost \n",
      "% di errori: 49.762499999999996 \n",
      "\n",
      "Iterazione  2000\n",
      "Loss corrente: 0.4992914484514749\n",
      "Alfa del modello corrente: 0.0014171040456512653\n",
      "Score di Adaboost \n",
      "% di errori: 35.699999999999996 \n",
      "\n",
      "Iterazione  4000\n",
      "Loss corrente: 0.49604734387983856\n",
      "Alfa del modello corrente: 0.007905476924591225\n",
      "Score di Adaboost \n",
      "% di errori: 30.65 \n",
      "\n",
      "Iterazione  6000\n",
      "Loss corrente: 0.4982383583631855\n",
      "Alfa del modello corrente: 0.0035232978525259603\n",
      "Score di Adaboost \n",
      "% di errori: 25.362499999999997 \n",
      "\n",
      "Iterazione  8000\n",
      "Loss corrente: 0.49932831019559626\n",
      "Alfa del modello corrente: 0.0013433804169267166\n",
      "Score di Adaboost \n",
      "% di errori: 22.325 \n",
      "\n",
      "SCORES ON TRAINING SET\n",
      "% di errori: 18.862499999999997 \n",
      "\n",
      "SCORES ON TEST SET\n",
      "% di errori: 43.5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rs = RandomLinearModel()\n",
    "a = AdaBoost(rs,10000)\n",
    "a.fit(X_train,y_train, 2000)\n",
    "\n",
    "y_train_ = a.predict(X_train)\n",
    "y_test_ = a.predict(X_test)\n",
    "\n",
    "print(\"SCORES ON TRAINING SET\")\n",
    "a.print_score(y_train, y_train_)\n",
    "\n",
    "print(\"SCORES ON TEST SET\")\n",
    "a.print_score(y_test, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write few paragraphs about what you think about the experiment and about the results you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
